{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import tensorly as tl\n",
    "\n",
    "from tddl.models.resnet import PA_ResNet18\n",
    "from tddl.models.resnet_lr import low_rank_resnet18\n",
    "from tddl.models.utils import count_parameters\n",
    "from tddl.factorizations import factorize_network, number_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')\n",
    "\n",
    "cuda = \"3\"\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cuda\n",
    "\n",
    "cpu = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = cpu\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = cpu\n",
    "os.environ[\"OMP_NUM_THREADS\"] = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jetzeschuurman/gitProjects/phd/tddl/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/jetzeschuurman/gitProjects/phd/tddl/notebooks/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"/local/jetzeschuurman/f_mnist/logs/parn_18_d0.5_256_sgd_l0.1_g0.1_sTrue/1633280228/cnn_best\"\n",
    "\n",
    "# load pretrained model\n",
    "pretrained_model = torch.load(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11170122"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_param = count_parameters(pretrained_model)\n",
    "pre_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': (0,\n",
       "  Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       " 'bn1': (1,\n",
       "  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'layer1': (2,\n",
       "  {'0': (3,\n",
       "    {'bn1': (4,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (5,\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (6,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (7,\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (8, Sequential())}),\n",
       "   '1': (9,\n",
       "    {'bn1': (10,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (11,\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (12,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (13,\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (14, Sequential())})}),\n",
       " 'layer2': (15,\n",
       "  {'0': (16,\n",
       "    {'bn1': (17,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (18,\n",
       "      Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n",
       "     'bn2': (19,\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (20,\n",
       "      Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (21,\n",
       "      {'0': (22,\n",
       "        Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False))})}),\n",
       "   '1': (23,\n",
       "    {'bn1': (24,\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (25,\n",
       "      Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (26,\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (27,\n",
       "      Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (28, Sequential())})}),\n",
       " 'layer3': (29,\n",
       "  {'0': (30,\n",
       "    {'bn1': (31,\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (32,\n",
       "      Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n",
       "     'bn2': (33,\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (34,\n",
       "      Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (35,\n",
       "      {'0': (36,\n",
       "        Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False))})}),\n",
       "   '1': (37,\n",
       "    {'bn1': (38,\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (39,\n",
       "      Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (40,\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (41,\n",
       "      Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (42, Sequential())})}),\n",
       " 'layer4': (43,\n",
       "  {'0': (44,\n",
       "    {'bn1': (45,\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (46,\n",
       "      Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n",
       "     'bn2': (47,\n",
       "      BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (48,\n",
       "      Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (49,\n",
       "      {'0': (50,\n",
       "        Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False))})}),\n",
       "   '1': (51,\n",
       "    {'bn1': (52,\n",
       "      BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (53,\n",
       "      Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (54,\n",
       "      BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (55,\n",
       "      Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (56, Sequential())})}),\n",
       " 'linear': (57, Linear(in_features=512, out_features=10, bias=True))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_layers(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(0, None)\n",
      "1 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(1, None)\n",
      "2 layer1 <class 'torch.nn.modules.container.Sequential'>\n",
      "(2, None)\n",
      "3 0 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(3, None)\n",
      "4 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(4, None)\n",
      "5 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(5, tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "6 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(6, tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "7 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(7, tensor(3.0027, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "8 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(8, tensor(3.0027, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "9 1 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(9, None)\n",
      "10 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(10, None)\n",
      "11 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(11, tensor(3.4912, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "12 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(12, tensor(3.4912, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "13 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(13, tensor(3.5323, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "14 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(14, tensor(3.5323, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "15 layer2 <class 'torch.nn.modules.container.Sequential'>\n",
      "(15, None)\n",
      "16 0 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(16, None)\n",
      "17 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(17, None)\n",
      "18 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(18, tensor(3.6274, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "19 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(19, tensor(3.6274, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "20 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(20, tensor(4.5748, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "21 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(21, tensor(4.5748, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "22 0 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(22, None)\n",
      "23 1 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(23, None)\n",
      "24 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(24, None)\n",
      "25 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(25, tensor(5.0622, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "26 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(26, tensor(5.0622, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "27 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(27, tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "28 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(28, tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "29 layer3 <class 'torch.nn.modules.container.Sequential'>\n",
      "(29, None)\n",
      "30 0 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(30, None)\n",
      "31 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(31, None)\n",
      "32 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(32, tensor(5.4734, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "33 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(33, tensor(5.4734, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "34 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(34, tensor(6.1732, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "35 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(35, tensor(6.1732, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "36 0 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(36, None)\n",
      "37 1 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(37, None)\n",
      "38 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(38, None)\n",
      "39 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(39, tensor(6.4285, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "40 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(40, tensor(6.4285, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "41 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(41, tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "42 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(42, tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "43 layer4 <class 'torch.nn.modules.container.Sequential'>\n",
      "(43, None)\n",
      "44 0 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(44, None)\n",
      "45 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(45, None)\n",
      "46 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(46, tensor(4.1123, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "47 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(47, tensor(4.1123, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "48 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(48, tensor(1.6981, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "49 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(49, tensor(1.6981, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "50 0 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(50, None)\n",
      "51 1 <class 'tddl.models.resnet.PreActBlock'>\n",
      "(51, None)\n",
      "52 bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(52, None)\n",
      "53 conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(53, tensor(1.3790, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "54 bn2 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "(54, tensor(1.3790, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "55 conv2 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "(55, tensor(1.1623, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "56 shortcut <class 'torch.nn.modules.container.Sequential'>\n",
      "(56, tensor(1.1623, device='cuda:0', grad_fn=<CopyBackwards>))\n",
      "57 linear <class 'torch.nn.modules.linear.Linear'>\n",
      "(57, None)\n"
     ]
    }
   ],
   "source": [
    "fact_model = copy.deepcopy(pretrained_model)\n",
    "\n",
    "# TODO: do I also consider the skip conneciton layers?\n",
    "# For now not\n",
    "layers = [5, 7, 11, 13, 18, 20, 25, 27, 32, 34, 39, 41, 46, 48, 53, 55]\n",
    "factorization='tucker'\n",
    "rank=0.5\n",
    "decompose_weights=True\n",
    "\n",
    "decomposition_kwargs = {'init': 'random'} if factorization == 'cp' else {}\n",
    "fixed_rank_modes = 'spatial' if factorization == 'tucker' else None\n",
    "\n",
    "output = factorize_network(\n",
    "    fact_model,\n",
    "    layers=layers,\n",
    "    factorization=factorization,\n",
    "    rank=rank,\n",
    "    decompose_weights=decompose_weights,\n",
    "    return_error=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': (0,\n",
       "  None,\n",
       "  Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       " 'bn1': (1,\n",
       "  None,\n",
       "  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'layer1': (2,\n",
       "  None,\n",
       "  {'0': (3,\n",
       "    None,\n",
       "    {'bn1': (4,\n",
       "      None,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (5,\n",
       "      tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (6,\n",
       "      tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (7,\n",
       "      tensor(3.0027, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (8,\n",
       "      tensor(3.0027, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Sequential())}),\n",
       "   '1': (9,\n",
       "    None,\n",
       "    {'bn1': (10,\n",
       "      None,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (11,\n",
       "      tensor(3.4912, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (12,\n",
       "      tensor(3.4912, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (13,\n",
       "      tensor(3.5323, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (14,\n",
       "      tensor(3.5323, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Sequential())})}),\n",
       " 'layer2': (15,\n",
       "  None,\n",
       "  {'0': (16,\n",
       "    None,\n",
       "    {'bn1': (17,\n",
       "      None,\n",
       "      BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (18,\n",
       "      tensor(3.6274, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n",
       "     'bn2': (19,\n",
       "      tensor(3.6274, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (20,\n",
       "      tensor(4.5748, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (21,\n",
       "      tensor(4.5748, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      {'0': (22,\n",
       "        None,\n",
       "        Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False))})}),\n",
       "   '1': (23,\n",
       "    None,\n",
       "    {'bn1': (24,\n",
       "      None,\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (25,\n",
       "      tensor(5.0622, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (26,\n",
       "      tensor(5.0622, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (27,\n",
       "      tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (28,\n",
       "      tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Sequential())})}),\n",
       " 'layer3': (29,\n",
       "  None,\n",
       "  {'0': (30,\n",
       "    None,\n",
       "    {'bn1': (31,\n",
       "      None,\n",
       "      BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (32,\n",
       "      tensor(5.4734, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n",
       "     'bn2': (33,\n",
       "      tensor(5.4734, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (34,\n",
       "      tensor(6.1732, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (35,\n",
       "      tensor(6.1732, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      {'0': (36,\n",
       "        None,\n",
       "        Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False))})}),\n",
       "   '1': (37,\n",
       "    None,\n",
       "    {'bn1': (38,\n",
       "      None,\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (39,\n",
       "      tensor(6.4285, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (40,\n",
       "      tensor(6.4285, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (41,\n",
       "      tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (42,\n",
       "      tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Sequential())})}),\n",
       " 'layer4': (43,\n",
       "  None,\n",
       "  {'0': (44,\n",
       "    None,\n",
       "    {'bn1': (45,\n",
       "      None,\n",
       "      BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (46,\n",
       "      tensor(4.1123, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n",
       "     'bn2': (47,\n",
       "      tensor(4.1123, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (48,\n",
       "      tensor(1.6981, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (49,\n",
       "      tensor(1.6981, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      {'0': (50,\n",
       "        None,\n",
       "        Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False))})}),\n",
       "   '1': (51,\n",
       "    None,\n",
       "    {'bn1': (52,\n",
       "      None,\n",
       "      BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv1': (53,\n",
       "      tensor(1.3790, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'bn2': (54,\n",
       "      tensor(1.3790, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "     'conv2': (55,\n",
       "      tensor(1.1623, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n",
       "     'shortcut': (56,\n",
       "      tensor(1.1623, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       "      Sequential())})}),\n",
       " 'linear': (57, None, Linear(in_features=512, out_features=10, bias=True))}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 (5, tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv2 (7, tensor(3.0027, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (11, tensor(3.4912, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv2 (13, tensor(3.5323, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (18, tensor(3.6274, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False))\n",
      "conv2 (20, tensor(4.5748, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (25, tensor(5.0622, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv2 (27, tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (32, tensor(5.4734, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False))\n",
      "conv2 (34, tensor(6.1732, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (39, tensor(6.4285, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv2 (41, tensor(4.8250, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (46, tensor(4.1123, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False))\n",
      "conv2 (48, tensor(1.6981, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv1 (53, tensor(1.3790, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      "conv2 (55, tensor(1.1623, device='cuda:0', grad_fn=<CopyBackwards>), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n"
     ]
    }
   ],
   "source": [
    "def outer(output, layers):\n",
    "    list_errors = []\n",
    "    \n",
    "    def parse_errors(d, layers, \n",
    "        # list_errors=None,\n",
    "    ):\n",
    "        nonlocal list_errors\n",
    "        # if list_errors is None:\n",
    "        #     list_errors = []\n",
    "\n",
    "        for k, v in d.items():\n",
    "            # print(v[0])\n",
    "            if isinstance(v[2], dict):\n",
    "                parse_errors(v[2], layers)\n",
    "            elif v[0] in layers:\n",
    "                print(k,v)\n",
    "                list_errors.append(v)\n",
    "\n",
    "    parse_errors(output, layers)\n",
    "    return list_errors\n",
    "\n",
    "list_errors = outer(output, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_errors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TuckerTensor(shape=(64, 64, 3, 3), rank=(39, 39, 3, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuckertensor = fact_model.layer1[0].conv1.weight\n",
    "tuckertensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx = tuckertensor.to_tensor()\n",
    "approx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = pretrained_model.layer1[0].conv1.weight\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9908, device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(approx-tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_error(\n",
    "    original, \n",
    "    approximation,\n",
    "    **kwargs,\n",
    "):\n",
    "    return torch.norm(original-approximation, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56ddcafc5f4a65ffc1eba06f4696d06fbf43c848b7a2cf81f3fe8a9e81fc5ea1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
