{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For baseline / factorized model\n",
    "\n",
    "# Load model\n",
    "# Hook given layer\n",
    "# Loop over data \n",
    "    # forward pass 1 obs -> stores \n",
    "    # Store feature embedding of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://forums.fast.ai/t/pytorch-best-way-to-get-at-intermediate-layers-in-vgg-and-resnet/5707/3\n",
    "\n",
    "# outputs= []\n",
    "# def hook(module, input, output):\n",
    "#     outputs.append(output)\n",
    "\n",
    "# baseline_model.layer1[0].conv1.register_forward_hook(hook)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# img, label = batch\n",
    "\n",
    "# out = baseline_model(img.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/bigdata/f_mnist/logs/baselines/1644945876/rn18_18_dNone_256_adam_l0.001_g0.1_sTrue')\n",
    "\n",
    "layer = 28\n",
    "logdir = path / f'features_layer{layer}_seed{seed}'\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "baseline_path = Path('/bigdata/f_mnist/logs/baselines/1644945876/rn18_18_dNone_256_adam_l0.001_g0.1_sTrue/cnn_best.pth')\n",
    "baseline_model = torch.load(path / \"cnn_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tddl.data.loaders import fmnist_stratified_loaders\n",
    "\n",
    "data_dir = Path(\"/bigdata/f_mnist\")\n",
    "batch_size = 1\n",
    "data_workers = 8\n",
    "\n",
    "train_loader, valid_loader, test_loader = fmnist_stratified_loaders(\n",
    "    path=data_dir,\n",
    "    batch_size=batch_size,\n",
    "    data_workers=data_workers,\n",
    "    valid_size=5000,\n",
    "    random_transform_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs= []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "\n",
    "def hook_network(\n",
    "    model, layers=[], exclude=[], verbose=False, \n",
    "    return_error=False, \n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Usage: factorize_network(model, layers=[6])\n",
    "    \"\"\"\n",
    "    i = -1\n",
    "    def nested_children(m: torch.nn.Module, **kwargs):\n",
    "        \"\"\"\n",
    "        layers: List of either\n",
    "            layer numbers (numbered according to number_layers(model))\n",
    "            layer names\n",
    "            module types\n",
    "        \"\"\"\n",
    "        nonlocal i\n",
    "        children = dict(m.named_children())\n",
    "        output = {}\n",
    "        error = None\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "        if children == {}:\n",
    "            return m\n",
    "        else:\n",
    "        # look for children from children... to the last child!\n",
    "            for name, child in children.items():\n",
    "                if verbose:\n",
    "                    print(i, name, type(child))\n",
    "                if name in exclude:\n",
    "                    i+=1\n",
    "                    continue\n",
    "                # if type(child) == torch.nn.modules.conv.Conv2d and i in layers:\n",
    "                if i in layers or name in layers or type(child) in layers:\n",
    "                    # if return_error:\n",
    "                    #     layer, error = factorize_layer(child, **kwargs)\n",
    "                    #     if verbose:\n",
    "                    #         print(error)\n",
    "                    # else:\n",
    "                    #     layer = \n",
    "                    m._modules[name].register_forward_hook(hook)\n",
    "                try:\n",
    "                    # if verbose and return_error:\n",
    "                    #     print((i, error))\n",
    "                    output[name] = (i, nested_children(child, **kwargs) )\n",
    "                except TypeError:\n",
    "                    output[name] = (i, nested_children(child, **kwargs) )\n",
    "        return output #, errors\n",
    "    out = nested_children(model, **kwargs)\n",
    "\n",
    "    if return_error:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = torch.load(baseline_path)\n",
    "hook_network(baseline_model, layers=[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55000it [02:57, 309.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for i, (img, label) in enumerate(tqdm(train_loader)):\n",
    "        out = baseline_model(img.cuda())\n",
    "        features = outputs.pop()\n",
    "        torch.save(features, logdir / f\"features_i{i}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.3680e-01,  5.4497e-01,  9.0979e-01,  ..., -6.5729e-02,\n",
       "            1.2381e+00,  1.8000e+00],\n",
       "          [ 1.2621e+00,  4.4105e-01,  9.1233e-01,  ..., -3.6674e+00,\n",
       "           -2.8421e+00, -1.4289e+00],\n",
       "          [ 5.1122e-01,  1.4222e-01,  4.4297e-01,  ..., -6.6928e-01,\n",
       "           -3.3594e+00, -3.5585e+00],\n",
       "          ...,\n",
       "          [ 3.5719e+00,  2.4761e+00, -4.2372e-01,  ..., -7.1231e-01,\n",
       "           -2.6409e+00,  2.2242e-01],\n",
       "          [ 6.3407e+00,  7.1744e+00,  7.1558e+00,  ...,  6.6040e+00,\n",
       "            5.1314e+00,  4.7735e+00],\n",
       "          [ 4.8656e-01,  1.6399e-01,  1.8810e+00,  ...,  7.6749e-01,\n",
       "            4.5924e-01,  6.5591e-01]],\n",
       "\n",
       "         [[ 1.9712e-01,  7.3736e-01,  5.7075e-01,  ..., -2.7825e+00,\n",
       "           -3.8005e+00, -2.9572e+00],\n",
       "          [ 7.3113e-02,  3.6040e-01,  3.0848e-01,  ..., -9.2888e-02,\n",
       "           -9.5780e-01,  4.9804e-01],\n",
       "          [-9.7015e-01, -1.3747e+00, -1.2999e+00,  ...,  1.8417e+00,\n",
       "            1.2893e+00,  2.9309e+00],\n",
       "          ...,\n",
       "          [ 3.5492e+00,  7.5920e+00,  8.4231e+00,  ...,  2.0227e+00,\n",
       "            1.5067e+00,  2.7629e+00],\n",
       "          [ 9.2776e-01,  4.4301e+00,  6.1006e+00,  ...,  2.1179e+00,\n",
       "            4.9397e+00,  3.9370e+00],\n",
       "          [-1.9904e+00, -1.5979e+00, -4.3830e-01,  ..., -1.0710e+00,\n",
       "            2.3385e+00,  1.5388e+00]],\n",
       "\n",
       "         [[ 1.0992e+00,  1.0144e+00,  1.0046e+00,  ...,  2.9115e+00,\n",
       "            2.2184e+00,  2.4059e+00],\n",
       "          [ 1.2539e+00,  8.6629e-01,  4.3913e-01,  ...,  3.2547e+00,\n",
       "            4.5178e+00,  4.0469e+00],\n",
       "          [ 1.8949e+00,  2.0137e+00,  1.4637e+00,  ...,  2.1667e+00,\n",
       "            5.5444e+00,  5.6361e+00],\n",
       "          ...,\n",
       "          [ 5.6075e+00,  2.4641e+00,  2.8716e-01,  ...,  5.6140e-01,\n",
       "            1.9375e+00,  2.3025e+00],\n",
       "          [ 4.7299e+00,  2.1630e+00,  2.6924e+00,  ...,  1.0330e+00,\n",
       "            2.2886e+00,  4.1510e+00],\n",
       "          [ 2.5283e+00,  1.8407e+00,  1.8301e+00,  ...,  9.4222e-01,\n",
       "            3.8697e+00,  4.4332e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1285e+00, -1.4054e+00, -1.3507e+00,  ..., -2.3147e-01,\n",
       "           -5.9784e-01, -1.4266e+00],\n",
       "          [-1.5489e+00, -1.5424e+00, -6.5588e-01,  ..., -3.2258e-01,\n",
       "           -2.0493e+00, -2.8653e+00],\n",
       "          [-1.6854e+00, -1.4887e+00, -6.5129e-01,  ..., -6.7063e+00,\n",
       "           -8.6551e+00, -9.0345e+00],\n",
       "          ...,\n",
       "          [-3.0986e+00, -3.5093e+00, -3.7855e+00,  ..., -1.6342e+00,\n",
       "           -3.3849e+00, -5.5479e+00],\n",
       "          [-2.5780e+00, -4.1003e+00, -4.8458e+00,  ..., -3.6258e+00,\n",
       "           -4.3283e+00, -6.9526e+00],\n",
       "          [-9.1247e-01, -2.9988e+00, -4.8983e+00,  ..., -2.7855e+00,\n",
       "           -4.7166e+00, -6.9602e+00]],\n",
       "\n",
       "         [[ 2.1123e+00,  2.1345e+00,  2.2505e+00,  ...,  2.1835e+00,\n",
       "            2.4465e+00,  1.6898e+00],\n",
       "          [ 1.6352e+00,  1.9556e+00,  2.1408e+00,  ...,  4.9839e-01,\n",
       "            3.1201e+00,  2.2261e+00],\n",
       "          [ 3.3179e-01,  9.5953e-01,  1.3856e+00,  ...,  1.0629e+00,\n",
       "            8.3148e-01, -6.1375e-01],\n",
       "          ...,\n",
       "          [ 2.0940e+00,  4.7793e-01,  4.3590e-01,  ...,  9.3950e-01,\n",
       "            8.7021e-01, -2.6845e+00],\n",
       "          [ 7.6678e-01,  5.1100e-01, -4.6160e-01,  ..., -7.8536e-02,\n",
       "           -4.3908e-01, -3.2622e+00],\n",
       "          [-2.3058e+00, -9.7359e-01, -8.0444e-01,  ..., -1.1149e+00,\n",
       "           -2.0183e-01, -5.3012e-01]],\n",
       "\n",
       "         [[ 1.0176e+00,  4.3247e-01,  2.7322e-01,  ...,  1.2619e+00,\n",
       "            2.4381e+00,  1.7172e+00],\n",
       "          [ 3.9262e-01, -4.8190e-01, -3.7059e-01,  ...,  2.0389e+00,\n",
       "            4.4337e+00,  1.5312e+00],\n",
       "          [-3.0698e-03,  8.0545e-02,  1.0828e-01,  ..., -3.1320e+00,\n",
       "           -1.8950e+00, -4.6209e+00],\n",
       "          ...,\n",
       "          [-2.5899e+00, -3.9882e+00, -3.2300e+00,  ..., -8.5341e-01,\n",
       "           -4.3933e+00, -3.8800e+00],\n",
       "          [-4.1013e+00, -5.3891e+00, -3.3548e+00,  ...,  1.7755e-01,\n",
       "           -4.0500e+00, -4.3411e+00],\n",
       "          [-2.5152e+00, -3.7255e+00, -3.4735e+00,  ...,  1.1133e-03,\n",
       "           -2.4450e+00, -2.5879e+00]]]], device='cuda:0',\n",
       "       grad_fn=<CudnnConvolutionBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if hooking features works for factorized model\n",
    "\n",
    "layer = 15 # from config file\n",
    "fact_model = torch.load(\"/bigdata/f_mnist/logs/decomposed_adam/1645014717/rn18-lr-[15]-cp-0.5-dTrue-iNone_bn_256_sgd_l1e-05_g0.0_sTrue/fact_model_best.pth\")\n",
    "hook_network(fact_model, layers=[layer])\n",
    "fact_model(img.cuda())\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56ddcafc5f4a65ffc1eba06f4696d06fbf43c848b7a2cf81f3fe8a9e81fc5ea1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
