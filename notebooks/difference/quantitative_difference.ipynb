{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of folders: 260\n",
      "260\n",
      "260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetzeschuurman/gitProjects/phd/tltorch/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "logdir = Path(\"/bigdata/cifar10/logs/decomposed\")\n",
    "import os\n",
    "from datetime import datetime\n",
    "from os.path import isdir\n",
    "\n",
    "folders = os.listdir(logdir)\n",
    "print(f\"number of folders: {len(folders)}\")\n",
    "# from datetime import datetime\n",
    "\n",
    "# dts = [datetime.fromtimestamp(int(x)) for x in folders]\n",
    "# print(max(dts))\n",
    "# print(min(dts))\n",
    "from tddl.post_processing.path_utils import logdir_to_paths\n",
    "\n",
    "paths = logdir_to_paths(logdir)\n",
    "\n",
    "print(len(paths))\n",
    "# baseline\n",
    "\n",
    "baseline_path = Path(\"/bigdata/cifar10/logs/baselines/1646668631/rn18_18_dNone_128_adam_l0.001_g0.1_w0.0_sTrue\")\n",
    "# baseline_model = torch.load(baseline_path / \"cnn_best.pth\")\n",
    "with open(baseline_path/'results.json') as json_file:\n",
    "    baseline_result = json.load(json_file)\n",
    "baseline_result\n",
    "from tddl.post_processing.factorized_model import process_factorized_networks\n",
    "\n",
    "# process_factorized_networks(paths, baseline_path)\n",
    "from tddl.post_processing.path_utils import paths_to_df\n",
    "\n",
    "df = paths_to_df(paths)\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "# df.groupby('rank').count()\n",
    "# rank=0.90: 10 observations\n",
    "# rank=0.75: 10 observstions\n",
    "\n",
    "# Exclude the few observations (layer=28, decomp={cp,tucker}) where rank is 0.75 or 0.90\n",
    "\n",
    "df = df[~df['rank'].isin(['0.75', '0.90'])]\n",
    "df['test_error'] = 1 - df.test_acc\n",
    "df['valid_error_before_ft'] = 1 - df.valid_acc_before_ft\n",
    "df['valid_error'] = 1 - df.valid_acc\n",
    "\n",
    "df['log_test_error'] = np.log(df.test_error)\n",
    "df['log_valid_error_before_ft'] = np.log(df.valid_error_before_ft)\n",
    "df['log_valid_error'] = np.log(df.valid_error)\n",
    "df['rank'] = df['rank'].astype(float, copy=False)\n",
    "# df['rank'].apply(float)\n",
    "df['rank'].unique()\n",
    "df['fact_rank'] = df['factorization'] + '-' + df['rank'].apply(str)\n",
    "df['fact_layers'] = df['factorization'] + '-' + df['layers'].apply(str)\n",
    "df['layers_fact'] = df['layers'].apply(str) + '-' + df['factorization'] \n",
    "df.head()\n",
    "df = df.astype({\n",
    "    'layers':\"category\",\n",
    "    'fact_layers':\"category\",\n",
    "    'layers_fact':\"category\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different layers\n",
    "\n",
    "In Figure 2 or 4 there are clusters of layers that seem to be separate from other layers. \\\n",
    "This resuls in seemingly different linear estimations of the relationship between approximation error and performance error. \\\n",
    "With the following we quantify the similarity / difference between layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test for difference between layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I am currently doing is estimating an intercept $\\alpha_{\\ell}$ and a coefficient $\\beta_\\ell$ per layer $\\ell$: \n",
    "\n",
    "$y_{i\\ell} = \\alpha_{\\ell} + \\beta_\\ell * x_{i\\ell} + \\varepsilon_{i\\ell}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the difference between layers, we estimate coefficients with respect to a specific layer, e.g. layer 15:\n",
    "\n",
    "$y_i = \\alpha_{15} + \\beta_{15} * w_i + \\sum_{\\ell \\in layers_{not15}} I_i * (\\alpha_{\\Delta \\ell} + \\beta_{\\Delta\\ell} * w_i) + \\varepsilon_i $\n",
    "\n",
    "where $I_i = 1$ if $i$ corresponds to layer $\\ell$ and $I_i = 1$ if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now layer $\\ell$ can be compared to layer 15 by means of an F-test. \n",
    "The test can be used to test if $\\alpha_{\\Delta \\ell}$ en $\\beta_{\\Delta \\ell}$ are equal to zero. \n",
    "If this holds these two layers have a similar relationship.\n",
    "\n",
    "This can be used to test for 15 vs 19, 15 vs 28, ..., 15 vs 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The estimated equation can then be based on 19:\n",
    "\n",
    "$y_i = \\alpha_{19} + \\beta_{19} * w_i + \\sum_{\\ell \\in layers_{not19}} I_i * (\\alpha_{\\Delta \\ell} + \\beta_{\\Delta\\ell} * w_i) + \\varepsilon_i $\n",
    "\n",
    "This can be used to test for 19 vs 28, ..., 19 vs 64.\n",
    "\n",
    "The process can be repeated to fill the upper triangle of a matrix that thas rows and columns corresponding to the layers [19, 28, 38, 41, 44, 60, 63]. Element i,j show whether layer i and layer j have a similar or significantly different relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'nr', 'relative_norm_weight', 'scaled_norm_weight',\n",
       "       'diff_norm_weight', 'layers', 'factorization', 'rank', 'valid_acc',\n",
       "       'valid_acc_before_ft', 'n_param_fact', 'test_acc', 'lr', 'optimizer',\n",
       "       'norm_diff', 'norm_b', 'n_b', 'relative_norm', 'scaled_norm',\n",
       "       'test_error', 'valid_error_before_ft', 'valid_error', 'log_test_error',\n",
       "       'log_valid_error_before_ft', 'log_valid_error', 'fact_rank',\n",
       "       'fact_layers', 'layers_fact'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('log_test_error ~ layers + layers:relative_norm_weight', data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>layers[T.19]</th>\n",
       "      <th>layers[T.28]</th>\n",
       "      <th>layers[T.38]</th>\n",
       "      <th>layers[T.41]</th>\n",
       "      <th>layers[T.44]</th>\n",
       "      <th>layers[T.60]</th>\n",
       "      <th>layers[T.63]</th>\n",
       "      <th>layers[15]:relative_norm_weight</th>\n",
       "      <th>layers[19]:relative_norm_weight</th>\n",
       "      <th>layers[28]:relative_norm_weight</th>\n",
       "      <th>layers[38]:relative_norm_weight</th>\n",
       "      <th>layers[41]:relative_norm_weight</th>\n",
       "      <th>layers[44]:relative_norm_weight</th>\n",
       "      <th>layers[60]:relative_norm_weight</th>\n",
       "      <th>layers[63]:relative_norm_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272814</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Intercept  layers[T.19]  layers[T.28]  layers[T.38]  layers[T.41]  \\\n",
       "0          1.0           0.0           0.0           0.0           0.0   \n",
       "1          1.0           0.0           0.0           0.0           0.0   \n",
       "2          1.0           0.0           0.0           0.0           0.0   \n",
       "3          1.0           0.0           1.0           0.0           0.0   \n",
       "5          1.0           0.0           0.0           0.0           0.0   \n",
       "..         ...           ...           ...           ...           ...   \n",
       "255        1.0           0.0           0.0           0.0           0.0   \n",
       "256        1.0           0.0           0.0           0.0           0.0   \n",
       "257        1.0           0.0           0.0           1.0           0.0   \n",
       "258        1.0           0.0           0.0           0.0           0.0   \n",
       "259        1.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     layers[T.44]  layers[T.60]  layers[T.63]  \\\n",
       "0             0.0           0.0           1.0   \n",
       "1             0.0           1.0           0.0   \n",
       "2             0.0           1.0           0.0   \n",
       "3             0.0           0.0           0.0   \n",
       "5             0.0           0.0           1.0   \n",
       "..            ...           ...           ...   \n",
       "255           1.0           0.0           0.0   \n",
       "256           0.0           0.0           1.0   \n",
       "257           0.0           0.0           0.0   \n",
       "258           0.0           0.0           0.0   \n",
       "259           1.0           0.0           0.0   \n",
       "\n",
       "     layers[15]:relative_norm_weight  layers[19]:relative_norm_weight  \\\n",
       "0                           0.000000                              0.0   \n",
       "1                           0.000000                              0.0   \n",
       "2                           0.000000                              0.0   \n",
       "3                           0.000000                              0.0   \n",
       "5                           0.000000                              0.0   \n",
       "..                               ...                              ...   \n",
       "255                         0.000000                              0.0   \n",
       "256                         0.000000                              0.0   \n",
       "257                         0.000000                              0.0   \n",
       "258                         0.798234                              0.0   \n",
       "259                         0.000000                              0.0   \n",
       "\n",
       "     layers[28]:relative_norm_weight  layers[38]:relative_norm_weight  \\\n",
       "0                           0.000000                         0.000000   \n",
       "1                           0.000000                         0.000000   \n",
       "2                           0.000000                         0.000000   \n",
       "3                           0.823151                         0.000000   \n",
       "5                           0.000000                         0.000000   \n",
       "..                               ...                              ...   \n",
       "255                         0.000000                         0.000000   \n",
       "256                         0.000000                         0.000000   \n",
       "257                         0.000000                         0.452808   \n",
       "258                         0.000000                         0.000000   \n",
       "259                         0.000000                         0.000000   \n",
       "\n",
       "     layers[41]:relative_norm_weight  layers[44]:relative_norm_weight  \\\n",
       "0                                0.0                         0.000000   \n",
       "1                                0.0                         0.000000   \n",
       "2                                0.0                         0.000000   \n",
       "3                                0.0                         0.000000   \n",
       "5                                0.0                         0.000000   \n",
       "..                               ...                              ...   \n",
       "255                              0.0                         0.447319   \n",
       "256                              0.0                         0.000000   \n",
       "257                              0.0                         0.000000   \n",
       "258                              0.0                         0.000000   \n",
       "259                              0.0                         0.738350   \n",
       "\n",
       "     layers[60]:relative_norm_weight  layers[63]:relative_norm_weight  \n",
       "0                           0.000000                         0.311843  \n",
       "1                           0.337100                         0.000000  \n",
       "2                           0.272814                         0.000000  \n",
       "3                           0.000000                         0.000000  \n",
       "5                           0.000000                         0.198279  \n",
       "..                               ...                              ...  \n",
       "255                         0.000000                         0.000000  \n",
       "256                         0.000000                         0.160246  \n",
       "257                         0.000000                         0.000000  \n",
       "258                         0.000000                         0.000000  \n",
       "259                         0.000000                         0.000000  \n",
       "\n",
       "[240 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         log_test_error   R-squared:                       0.816\n",
      "Model:                            OLS   Adj. R-squared:                  0.803\n",
      "Method:                 Least Squares   F-statistic:                     66.14\n",
      "Date:                Wed, 15 Jun 2022   Prob (F-statistic):           1.91e-73\n",
      "Time:                        09:47:35   Log-Likelihood:                 657.61\n",
      "No. Observations:                 240   AIC:                            -1283.\n",
      "Df Residuals:                     224   BIC:                            -1228.\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================================\n",
      "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Intercept                          -2.4891      0.011   -233.684      0.000      -2.510      -2.468\n",
      "layers[T.19]                       -0.0444      0.015     -3.012      0.003      -0.074      -0.015\n",
      "layers[T.28]                       -0.1055      0.016     -6.410      0.000      -0.138      -0.073\n",
      "layers[T.38]                        0.0089      0.016      0.555      0.579      -0.023       0.041\n",
      "layers[T.41]                       -0.0457      0.020     -2.309      0.022      -0.085      -0.007\n",
      "layers[T.44]                        0.0196      0.016      1.224      0.222      -0.012       0.051\n",
      "layers[T.60]                        0.0251      0.017      1.509      0.133      -0.008       0.058\n",
      "layers[T.63]                        0.0325      0.017      1.903      0.058      -0.001       0.066\n",
      "layers[15]:relative_norm_weight     0.0988      0.018      5.518      0.000       0.063       0.134\n",
      "layers[19]:relative_norm_weight     0.1980      0.018     10.853      0.000       0.162       0.234\n",
      "layers[28]:relative_norm_weight     0.3351      0.019     17.357      0.000       0.297       0.373\n",
      "layers[38]:relative_norm_weight     0.0566      0.020      2.766      0.006       0.016       0.097\n",
      "layers[41]:relative_norm_weight     0.1266      0.022      5.722      0.000       0.083       0.170\n",
      "layers[44]:relative_norm_weight     0.0284      0.021      1.362      0.174      -0.013       0.069\n",
      "layers[60]:relative_norm_weight     0.0242      0.047      0.510      0.611      -0.069       0.118\n",
      "layers[63]:relative_norm_weight    -0.0094      0.053     -0.178      0.859      -0.113       0.095\n",
      "==============================================================================\n",
      "Omnibus:                       10.913   Durbin-Watson:                   2.155\n",
      "Prob(Omnibus):                  0.004   Jarque-Bera (JB):               14.720\n",
      "Skew:                           0.331   Prob(JB):                     0.000636\n",
      "Kurtosis:                       4.016   Cond. No.                         55.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "res = sm.OLS(y, X).fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=10.64941585426433, p=3.8184254634975756e-05, df_denom=224, df_num=2>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = [[\n",
    "    0, 1, 0, 0, 0, 0, 0, 0, \n",
    "    0, 0, 0, 0, 0, 0, 0, 0,\n",
    "    ],[\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, \n",
    "    1, -1, 0, 0, 0, 0, 0, 0,\n",
    "]]\n",
    "res.f_test(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 19\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0.  0.  0.]]\n",
      "<F test: F=10.64941585426433, p=3.8184254634975756e-05, df_denom=224, df_num=2>\n",
      "15 28\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0. -1.  0.  0.  0.  0.  0.]]\n",
      "<F test: F=78.7379286881748, p=1.269017414267539e-26, df_denom=224, df_num=2>\n",
      "15 38\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. -1.  0.  0.  0.  0.]]\n",
      "<F test: F=7.847615792708168, p=0.0005080727224531395, df_denom=224, df_num=2>\n",
      "15 41\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0.]]\n",
      "<F test: F=16.179613873505225, p=2.733349286828302e-07, df_denom=224, df_num=2>\n",
      "15 44\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. -1.  0.  0.]]\n",
      "<F test: F=14.783840025476847, p=9.316688326950524e-07, df_denom=224, df_num=2>\n",
      "15 60\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. -1.  0.]]\n",
      "<F test: F=1.1631443093674656, p=0.3143821328528141, df_denom=224, df_num=2>\n",
      "15 63\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. -1.]]\n",
      "<F test: F=1.933732072958725, p=0.1470138655503612, df_denom=224, df_num=2>\n",
      "19 28\n",
      "[[ 0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0.  0.]]\n",
      "<F test: F=23.1161507238236, p=7.4680108454631e-10, df_denom=224, df_num=2>\n",
      "19 38\n",
      "[[ 0.  1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. -1.  0.  0.  0.  0.]]\n",
      "<F test: F=31.407541671098368, p=9.47012121745523e-13, df_denom=224, df_num=2>\n",
      "19 41\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. -1.  0.  0.  0.]]\n",
      "<F test: F=48.144952840791035, p=4.045142986098752e-18, df_denom=224, df_num=2>\n",
      "19 44\n",
      "[[ 0.  1.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.  0.  0.]]\n",
      "<F test: F=42.54369977775897, p=2.181083908788071e-16, df_denom=224, df_num=2>\n",
      "19 60\n",
      "[[ 0.  1.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. -1.  0.]]\n",
      "<F test: F=9.931032397221106, p=7.372629769155949e-05, df_denom=224, df_num=2>\n",
      "19 63\n",
      "[[ 0.  1.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]]\n",
      "<F test: F=11.321622453757042, p=2.070259577123515e-05, df_denom=224, df_num=2>\n",
      "28 38\n",
      "[[ 0.  0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0.]]\n",
      "<F test: F=125.93226610400072, p=2.2368886075235306e-37, df_denom=224, df_num=2>\n",
      "28 41\n",
      "[[ 0.  0.  1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. -1.  0.  0.  0.]]\n",
      "<F test: F=205.91183342305703, p=1.7950181640774958e-51, df_denom=224, df_num=2>\n",
      "28 44\n",
      "[[ 0.  0.  1.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. -1.  0.  0.]]\n",
      "<F test: F=143.7722161052123, p=6.805209570247446e-41, df_denom=224, df_num=2>\n",
      "28 60\n",
      "[[ 0.  0.  1.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.  0.]]\n",
      "<F test: F=27.246316656559753, p=2.5623729590287163e-11, df_denom=224, df_num=2>\n",
      "28 63\n",
      "[[ 0.  0.  1.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. -1.]]\n",
      "<F test: F=29.289219056531078, p=5.0142706956934326e-12, df_denom=224, df_num=2>\n",
      "38 41\n",
      "[[ 0.  0.  0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.  0.  0.  0.]]\n",
      "<F test: F=4.0813586130953174, p=0.018156013912566014, df_denom=224, df_num=2>\n",
      "38 44\n",
      "[[ 0.  0.  0.  1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. -1.  0.  0.]]\n",
      "<F test: F=1.2527219867433568, p=0.2877198733952031, df_denom=224, df_num=2>\n",
      "38 60\n",
      "[[ 0.  0.  0.  1.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. -1.  0.]]\n",
      "<F test: F=0.5581110572057869, p=0.5730827895084001, df_denom=224, df_num=2>\n",
      "38 63\n",
      "[[ 0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.]]\n",
      "<F test: F=0.8603578338196112, p=0.4244037118866105, df_denom=224, df_num=2>\n",
      "41 44\n",
      "[[ 0.  0.  0.  0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]]\n",
      "<F test: F=5.24953883036345, p=0.0059152145241592805, df_denom=224, df_num=2>\n",
      "41 60\n",
      "[[ 0.  0.  0.  0.  1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. -1.  0.]]\n",
      "<F test: F=7.52348123149825, p=0.0006881075836319494, df_denom=224, df_num=2>\n",
      "41 63\n",
      "[[ 0.  0.  0.  0.  1.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. -1.]]\n",
      "<F test: F=7.817572838165864, p=0.0005225412116621786, df_denom=224, df_num=2>\n",
      "44 60\n",
      "[[ 0.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.  0.]]\n",
      "<F test: F=0.1819138108365985, p=0.8337962294805387, df_denom=224, df_num=2>\n",
      "44 63\n",
      "[[ 0.  0.  0.  0.  0.  1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. -1.]]\n",
      "<F test: F=0.25997883458804294, p=0.7713002405781977, df_denom=224, df_num=2>\n",
      "60 63\n",
      "[[ 0.  0.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.]]\n",
      "<F test: F=0.14411079187426967, p=0.8658720259336805, df_denom=224, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layers = [15,19,28,38,41,44,60,63]\n",
    "p_values = np.zeros((len(layers), len(layers)))\n",
    "# print(layers)\n",
    "for i,l in enumerate(layers):\n",
    "    for j,k in enumerate(layers[i+1:]):\n",
    "        print(l,k)\n",
    "        R = np.zeros((2,16))\n",
    "\n",
    "        if i == 0:\n",
    "            R[0,j+1] = 1 \n",
    "\n",
    "        else:\n",
    "            R[0,i] = 1\n",
    "            R[0,j+i+1] = -1\n",
    "\n",
    "        R[1,8+i] = 1\n",
    "        R[1,8+j+i+1] = -1\n",
    "\n",
    "        print(R)\n",
    "        f = res.f_test(R)\n",
    "        print(f)\n",
    "        p_values[i+j,i] = f.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.81842546e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.26901741e-26, 7.46801085e-10, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.08072722e-04, 9.47012122e-13, 2.23688861e-37, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.73334929e-07, 4.04514299e-18, 1.79501816e-51, 1.81560139e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.31668833e-07, 2.18108391e-16, 6.80520957e-41, 2.87719873e-01,\n",
       "        5.91521452e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.14382133e-01, 7.37262977e-05, 2.56237296e-11, 5.73082790e-01,\n",
       "        6.88107584e-04, 8.33796229e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.47013866e-01, 2.07025958e-05, 5.01427070e-12, 4.24403712e-01,\n",
       "        5.22541212e-04, 7.71300241e-01, 8.65872026e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = np.zeros_like(p_values)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(pd.DataFrame(data=p_values,columns=layers, index=layers), mask=mask, vmax=.3, square=True,  cmap=\"YlGnBu\")\n",
    "    plt.savefig(\"../../plots/cifar10/rn18/f_test_similarity_layers\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: bootstrapping goodness of fit measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute two linear equations, one based on all layers:\n",
    "\n",
    "$y_{i} = \\alpha + \\beta * x_{i} + \\varepsilon_{i}$\n",
    "\n",
    "and the other on only the observations of layer $\\ell$:\n",
    "\n",
    "$y_{i\\ell} = \\alpha_{\\ell} + \\beta_\\ell * x_{i\\ell} + \\varepsilon_{i\\ell}$\n",
    "\n",
    "Calculate the MSEs for observations of layer $\\ell$ based on both equations.\n",
    "Comparing the difference between the two MSEs gives an indication of how well layer $\\ell$ can be described by fitting on all observations versus only layer $\\ell$.\n",
    "\n",
    "The limitation is that this is one layer versus the average.\n",
    "\n",
    "\n",
    "### MSE\n",
    "Average squared difference between predictions and true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$y_{i\\ell} = \\alpha_{\\ell} + \\beta_\\ell * x_{i\\ell} + \\varepsilon_{i\\ell}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'nr', 'relative_norm_weight', 'scaled_norm_weight',\n",
       "       'diff_norm_weight', 'layers', 'factorization', 'rank', 'valid_acc',\n",
       "       'valid_acc_before_ft', 'n_param_fact', 'test_acc', 'lr', 'optimizer',\n",
       "       'norm_diff', 'norm_b', 'n_b', 'relative_norm', 'scaled_norm',\n",
       "       'test_error', 'valid_error_before_ft', 'valid_error', 'log_test_error',\n",
       "       'log_valid_error_before_ft', 'log_valid_error', 'fact_rank',\n",
       "       'fact_layers', 'layers_fact'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('log_test_error ~ factorization:layers + factorization:layers:relative_norm_weight', data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>layers[T.19]</th>\n",
       "      <th>layers[T.28]</th>\n",
       "      <th>layers[T.38]</th>\n",
       "      <th>layers[T.41]</th>\n",
       "      <th>layers[T.44]</th>\n",
       "      <th>layers[T.60]</th>\n",
       "      <th>layers[T.63]</th>\n",
       "      <th>factorization[T.tucker]:layers[15]</th>\n",
       "      <th>factorization[T.tucker]:layers[19]</th>\n",
       "      <th>...</th>\n",
       "      <th>factorization[cp]:layers[38]:relative_norm_weight</th>\n",
       "      <th>factorization[tucker]:layers[38]:relative_norm_weight</th>\n",
       "      <th>factorization[cp]:layers[41]:relative_norm_weight</th>\n",
       "      <th>factorization[tucker]:layers[41]:relative_norm_weight</th>\n",
       "      <th>factorization[cp]:layers[44]:relative_norm_weight</th>\n",
       "      <th>factorization[tucker]:layers[44]:relative_norm_weight</th>\n",
       "      <th>factorization[cp]:layers[60]:relative_norm_weight</th>\n",
       "      <th>factorization[tucker]:layers[60]:relative_norm_weight</th>\n",
       "      <th>factorization[cp]:layers[63]:relative_norm_weight</th>\n",
       "      <th>factorization[tucker]:layers[63]:relative_norm_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311843</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Intercept  layers[T.19]  layers[T.28]  layers[T.38]  layers[T.41]  \\\n",
       "0          1.0           0.0           0.0           0.0           0.0   \n",
       "1          1.0           0.0           0.0           0.0           0.0   \n",
       "2          1.0           0.0           0.0           0.0           0.0   \n",
       "3          1.0           0.0           1.0           0.0           0.0   \n",
       "5          1.0           0.0           0.0           0.0           0.0   \n",
       "..         ...           ...           ...           ...           ...   \n",
       "255        1.0           0.0           0.0           0.0           0.0   \n",
       "256        1.0           0.0           0.0           0.0           0.0   \n",
       "257        1.0           0.0           0.0           1.0           0.0   \n",
       "258        1.0           0.0           0.0           0.0           0.0   \n",
       "259        1.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     layers[T.44]  layers[T.60]  layers[T.63]  \\\n",
       "0             0.0           0.0           1.0   \n",
       "1             0.0           1.0           0.0   \n",
       "2             0.0           1.0           0.0   \n",
       "3             0.0           0.0           0.0   \n",
       "5             0.0           0.0           1.0   \n",
       "..            ...           ...           ...   \n",
       "255           1.0           0.0           0.0   \n",
       "256           0.0           0.0           1.0   \n",
       "257           0.0           0.0           0.0   \n",
       "258           0.0           0.0           0.0   \n",
       "259           1.0           0.0           0.0   \n",
       "\n",
       "     factorization[T.tucker]:layers[15]  factorization[T.tucker]:layers[19]  \\\n",
       "0                                   0.0                                 0.0   \n",
       "1                                   0.0                                 0.0   \n",
       "2                                   0.0                                 0.0   \n",
       "3                                   0.0                                 0.0   \n",
       "5                                   0.0                                 0.0   \n",
       "..                                  ...                                 ...   \n",
       "255                                 0.0                                 0.0   \n",
       "256                                 0.0                                 0.0   \n",
       "257                                 0.0                                 0.0   \n",
       "258                                 1.0                                 0.0   \n",
       "259                                 0.0                                 0.0   \n",
       "\n",
       "     ...  factorization[cp]:layers[38]:relative_norm_weight  \\\n",
       "0    ...                                                0.0   \n",
       "1    ...                                                0.0   \n",
       "2    ...                                                0.0   \n",
       "3    ...                                                0.0   \n",
       "5    ...                                                0.0   \n",
       "..   ...                                                ...   \n",
       "255  ...                                                0.0   \n",
       "256  ...                                                0.0   \n",
       "257  ...                                                0.0   \n",
       "258  ...                                                0.0   \n",
       "259  ...                                                0.0   \n",
       "\n",
       "     factorization[tucker]:layers[38]:relative_norm_weight  \\\n",
       "0                                             0.000000       \n",
       "1                                             0.000000       \n",
       "2                                             0.000000       \n",
       "3                                             0.000000       \n",
       "5                                             0.000000       \n",
       "..                                                 ...       \n",
       "255                                           0.000000       \n",
       "256                                           0.000000       \n",
       "257                                           0.452808       \n",
       "258                                           0.000000       \n",
       "259                                           0.000000       \n",
       "\n",
       "     factorization[cp]:layers[41]:relative_norm_weight  \\\n",
       "0                                                  0.0   \n",
       "1                                                  0.0   \n",
       "2                                                  0.0   \n",
       "3                                                  0.0   \n",
       "5                                                  0.0   \n",
       "..                                                 ...   \n",
       "255                                                0.0   \n",
       "256                                                0.0   \n",
       "257                                                0.0   \n",
       "258                                                0.0   \n",
       "259                                                0.0   \n",
       "\n",
       "     factorization[tucker]:layers[41]:relative_norm_weight  \\\n",
       "0                                                  0.0       \n",
       "1                                                  0.0       \n",
       "2                                                  0.0       \n",
       "3                                                  0.0       \n",
       "5                                                  0.0       \n",
       "..                                                 ...       \n",
       "255                                                0.0       \n",
       "256                                                0.0       \n",
       "257                                                0.0       \n",
       "258                                                0.0       \n",
       "259                                                0.0       \n",
       "\n",
       "     factorization[cp]:layers[44]:relative_norm_weight  \\\n",
       "0                                                  0.0   \n",
       "1                                                  0.0   \n",
       "2                                                  0.0   \n",
       "3                                                  0.0   \n",
       "5                                                  0.0   \n",
       "..                                                 ...   \n",
       "255                                                0.0   \n",
       "256                                                0.0   \n",
       "257                                                0.0   \n",
       "258                                                0.0   \n",
       "259                                                0.0   \n",
       "\n",
       "     factorization[tucker]:layers[44]:relative_norm_weight  \\\n",
       "0                                             0.000000       \n",
       "1                                             0.000000       \n",
       "2                                             0.000000       \n",
       "3                                             0.000000       \n",
       "5                                             0.000000       \n",
       "..                                                 ...       \n",
       "255                                           0.447319       \n",
       "256                                           0.000000       \n",
       "257                                           0.000000       \n",
       "258                                           0.000000       \n",
       "259                                           0.738350       \n",
       "\n",
       "     factorization[cp]:layers[60]:relative_norm_weight  \\\n",
       "0                                                  0.0   \n",
       "1                                                  0.0   \n",
       "2                                                  0.0   \n",
       "3                                                  0.0   \n",
       "5                                                  0.0   \n",
       "..                                                 ...   \n",
       "255                                                0.0   \n",
       "256                                                0.0   \n",
       "257                                                0.0   \n",
       "258                                                0.0   \n",
       "259                                                0.0   \n",
       "\n",
       "     factorization[tucker]:layers[60]:relative_norm_weight  \\\n",
       "0                                             0.000000       \n",
       "1                                             0.337100       \n",
       "2                                             0.272814       \n",
       "3                                             0.000000       \n",
       "5                                             0.000000       \n",
       "..                                                 ...       \n",
       "255                                           0.000000       \n",
       "256                                           0.000000       \n",
       "257                                           0.000000       \n",
       "258                                           0.000000       \n",
       "259                                           0.000000       \n",
       "\n",
       "     factorization[cp]:layers[63]:relative_norm_weight  \\\n",
       "0                                             0.311843   \n",
       "1                                             0.000000   \n",
       "2                                             0.000000   \n",
       "3                                             0.000000   \n",
       "5                                             0.000000   \n",
       "..                                                 ...   \n",
       "255                                           0.000000   \n",
       "256                                           0.160246   \n",
       "257                                           0.000000   \n",
       "258                                           0.000000   \n",
       "259                                           0.000000   \n",
       "\n",
       "     factorization[tucker]:layers[63]:relative_norm_weight  \n",
       "0                                             0.000000      \n",
       "1                                             0.000000      \n",
       "2                                             0.000000      \n",
       "3                                             0.000000      \n",
       "5                                             0.198279      \n",
       "..                                                 ...      \n",
       "255                                           0.000000      \n",
       "256                                           0.000000      \n",
       "257                                           0.000000      \n",
       "258                                           0.000000      \n",
       "259                                           0.000000      \n",
       "\n",
       "[240 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56ddcafc5f4a65ffc1eba06f4696d06fbf43c848b7a2cf81f3fe8a9e81fc5ea1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
