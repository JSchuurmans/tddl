{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetzeschuurman/gitProjects/phd/tltorch/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 38,\n",
       " 'best_train_acc': 0.9749333333333333,\n",
       " 'best_valid_acc': 0.9078,\n",
       " 'best_valid_loss': 0.003568341651931405,\n",
       " 'test_acc': 0.891,\n",
       " 'test_loss': 0.004007616302371025,\n",
       " 'n_param': 557642,\n",
       " 'model_name': 'gar_18_dNone_128_sgd_l0.1_g0.1_w0.0_sTrue'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "baseline_path = Path(\"/bigdata/cifar10/logs/garipov/baselines/1647358615/gar_18_dNone_128_sgd_l0.1_g0.1_w0.0_sTrue\")\n",
    "baseline_model = torch.load(baseline_path / \"cnn_best.pth\")\n",
    "with open(baseline_path/'results.json') as json_file:\n",
    "    baseline_result = json.load(json_file)\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaripovNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tddl.factorizations import number_layers, listify_numbered_layers, get_weights\n",
    "\n",
    "numbered_layers = number_layers(baseline_model)\n",
    "gar_layers = [2,4,6,8,10]\n",
    "layers = listify_numbered_layers(numbered_layers, layer_nrs=gar_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_weights(baseline_model, gar_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetzeschuurman/gitProjects/phd/tddl/venv/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.\n",
      "  warnings.warn('In partial_svd: converting to NumPy.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(FactorizedConv(\n",
       "   in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(23, 23, 3, 3), order=2, padding=[1, 1], \n",
       "   (weight): TuckerTensor(shape=(64, 64, 3, 3), rank=(23, 23, 3, 3))\n",
       " ),\n",
       " 0.203125,\n",
       " tensor(0.5003, device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tddl.dbs import find_rank_given_error\n",
    "\n",
    "find_rank_given_error(layers[0][2], desired_error = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8260236135181975, [0.203125, 0.28125, 0.4375, 0.375, 0.02734375])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tddl.dbs import compress_layers_with_desired_error\n",
    "\n",
    "compress_layers_with_desired_error(layers, desired_error=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetzeschuurman/gitProjects/phd/tddl/venv/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.\n",
      "  warnings.warn('In partial_svd: converting to NumPy.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2735034305163528\n",
      "0.22649656948364721\n",
      "True\n",
      "----------\n",
      "0.25\n",
      "0.6774041410080303\n",
      "-0.17740414100803026\n",
      "True\n",
      "----------\n",
      "0.375\n",
      "0.4269280290939348\n",
      "0.07307197090606521\n",
      "True\n",
      "----------\n",
      "0.3125\n",
      "0.5415122964195667\n",
      "-0.04151229641956666\n",
      "True\n",
      "----------\n",
      "0.34375\n",
      "0.48262146681921375\n",
      "0.01737853318078625\n",
      "True\n",
      "----------\n",
      "0.328125\n",
      "0.5128182597437065\n",
      "-0.01281825974370654\n",
      "True\n",
      "----------\n",
      "0.3359375\n",
      "0.5011727954494102\n",
      "-0.00117279544941018\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from tddl.dbs import find_error_given_c\n",
    "from tddl.utils.model_stats import count_parameters\n",
    "\n",
    "baseline_count = count_parameters(baseline_model)\n",
    "ranks, c, error = find_error_given_c(layers, desired_c = 0.5, baseline_count=baseline_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4375, 0.5625, 0.75, 0.65625, 0.09375]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "rank = 0.5\n",
    "errors = np.array([])\n",
    "for layer in layers:\n",
    "    with torch.no_grad():\n",
    "        fact_layer, error = factorize_layer(layer[2], 'tucker', rank, return_error=True)\n",
    "    errors = np.append(errors, float(error.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = errors.mean()\n",
    "max_error = errors.max()\n",
    "min_error = errors.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35240626633167266\n",
      "-0.41636698440207964\n",
      "0.9163669844020796\n",
      "True\n",
      "----------\n",
      "0.4085878893733025\n",
      "-0.1684683708838821\n",
      "0.6684683708838821\n",
      "True\n",
      "----------\n",
      "0.43667870089411737\n",
      "-0.07165294627383023\n",
      "0.5716529462738302\n",
      "True\n",
      "----------\n",
      "0.4507241066545248\n",
      "0.0006878249566724115\n",
      "0.4993121750433276\n",
      "True\n",
      "----------\n",
      "0.4577468095347285\n",
      "0.035934792027729645\n",
      "0.46406520797227035\n",
      "True\n",
      "----------\n",
      "0.4612581609748304\n",
      "0.043224653379549394\n",
      "0.4567753466204506\n",
      "True\n",
      "----------\n",
      "0.46301383669488133\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46389167455490676\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4643305934849195\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4645500529499259\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4646597826824291\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46471464754868064\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46474207998180644\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647557961983694\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647626543066508\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647660833607915\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647677978878619\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647686551513971\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476908378316467\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476929809904843\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476940525699034\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647694588359613\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647694856254468\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647694990201895\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476950571756087\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695090662466\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695107405894\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695115777608\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951199634653\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695122056394\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951231028585\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951236260905\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695123887706\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951240185144\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124083918\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241166203\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124132971\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124141147\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241452347\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241472786\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241483005\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124148812\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124149067\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124149195\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241492587\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241492903\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493064\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124149315\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493186\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493203\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493214\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.4647695124149322\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n",
      "0.06800259965337951\n",
      "0.4319974003466205\n",
      "True\n",
      "----------\n",
      "0.46476951241493225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_406364/2323451973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_error_given_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_406364/2046359927.py\u001b[0m in \u001b[0;36mfind_error_given_c\u001b[0;34m(layers, desired_c, error, tollerance, max_error, min_error, max_iter, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_layers_with_desired_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtollerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtollerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired_c\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_406364/3590224019.py\u001b[0m in \u001b[0;36mcompress_layers_with_desired_error\u001b[0;34m(layers, desired_error, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mparam_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mfact_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_rank_given_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesired_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfact_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_406364/1777859090.py\u001b[0m in \u001b[0;36mfind_rank_given_error\u001b[0;34m(layer, desired_error, rank, tollerance, max_rank, min_rank, max_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# print(rank)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mfact_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tucker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# print(error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcriteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired_error\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtollerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tddl/src/tddl/factorizations.py\u001b[0m in \u001b[0;36mfactorize_layer\u001b[0;34m(module, factorization, rank, decompose_weights, init_std, return_error, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         fact_module = tltorch.FactorizedConv.from_conv(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tltorch/tltorch/factorized_layers/factorized_convolution.py\u001b[0m in \u001b[0;36mfrom_conv\u001b[0;34m(cls, conv_layer, rank, implementation, factorization, decompose_weights, decomposition_kwargs, fixed_rank_modes, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mkernel_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecomposition_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tltorch/tltorch/factorized_tensors/factorized_tensors.py\u001b[0m in \u001b[0;36minit_from_tensor\u001b[0;34m(self, tensor, unsqueezed_modes, unsqueezed_init, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtucker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0munsqueezed_modes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tddl/venv/lib/python3.8/site-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mtucker\u001b[0;34m(tensor, rank, fixed_factors, n_iter_max, init, svd, tol, random_state, mask, verbose)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_tucker_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         core, factors = partial_tucker(tensor, modes, rank=rank, n_iter_max=n_iter_max, init=init,\n\u001b[0m\u001b[1;32m    206\u001b[0m                             svd=svd, tol=tol, random_state=random_state, mask=mask, verbose=verbose)\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTuckerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tddl/venv/lib/python3.8/site-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mpartial_tucker\u001b[0;34m(tensor, modes, rank, n_iter_max, init, tol, svd, random_state, verbose, mask)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mcore_approximation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_mode_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0meigenvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore_approximation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigenvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tddl/venv/lib/python3.8/site-packages/tensorly/backend/core.py\u001b[0m in \u001b[0;36mpartial_svd\u001b[0;34m(self, matrix, n_eigenvecs, random_state, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mfull_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_eigenvecs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_eigenvecs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0;31m# If n_eigenvecs == min_dim, we don't want full_matrices=True, it's super slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitProjects/phd/tddl/venv/lib/python3.8/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[1;32m    126\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "find_error_given_c(layers, desired_c = 0.5, error=error, max_error=max_error, min_error=min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.4375\n",
      "4\n",
      "0.5625\n",
      "6\n",
      "0.75\n",
      "8\n",
      "0.65625\n",
      "10\n",
      "0.09375\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from tddl.factorizations import factorize_network, factorize_network_with_ranks\n",
    "\n",
    "factorization = 'tucker'\n",
    "\n",
    "\n",
    "fact_model = copy.deepcopy(baseline_model)\n",
    "\n",
    "layer_nrs = [layer[1] for layer in layers]\n",
    "\n",
    "fact_model = factorize_network_with_ranks(fact_model, layer_nrs, ranks, factorization=factorization)\n",
    "\n",
    "# for layer, rank in zip(layer_nrs, ranks):\n",
    "#         print(layer)\n",
    "#         print(rank)\n",
    "#         layer_nr = layer\n",
    "#         factorize_network(fact_model, layers=[layer_nr], rank=rank, factorization=factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': (0,\n",
       "  Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " 'conv1_bn': (1,\n",
       "  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'conv2': (2,\n",
       "  FactorizedConv(\n",
       "    in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(36, 36, 3, 3), order=2, padding=[1, 1], \n",
       "    (weight): TuckerTensor(shape=(64, 64, 3, 3), rank=(36, 36, 3, 3))\n",
       "  )),\n",
       " 'conv2_bn': (3,\n",
       "  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'conv3': (4,\n",
       "  FactorizedConv(\n",
       "    in_channels=64, out_channels=128, kernel_size=(3, 3), rank=(80, 40, 3, 3), order=2, padding=[1, 1], \n",
       "    (weight): TuckerTensor(shape=(128, 64, 3, 3), rank=(80, 40, 3, 3))\n",
       "  )),\n",
       " 'conv3_bn': (5,\n",
       "  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'conv4': (6,\n",
       "  FactorizedConv(\n",
       "    in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(98, 98, 3, 3), order=2, padding=[1, 1], \n",
       "    (weight): TuckerTensor(shape=(128, 128, 3, 3), rank=(98, 98, 3, 3))\n",
       "  )),\n",
       " 'conv4_bn': (7,\n",
       "  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'conv5': (8,\n",
       "  FactorizedConv(\n",
       "    in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(90, 90, 3, 3), order=2, padding=[1, 1], \n",
       "    (weight): TuckerTensor(shape=(128, 128, 3, 3), rank=(90, 90, 3, 3))\n",
       "  )),\n",
       " 'conv5_bn': (9,\n",
       "  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       " 'conv6': (10,\n",
       "  FactorizedConv(\n",
       "    in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(27, 27, 3, 3), order=2, padding=[1, 1], \n",
       "    (weight): TuckerTensor(shape=(128, 128, 3, 3), rank=(27, 27, 3, 3))\n",
       "  )),\n",
       " 'fc1': (11, Linear(in_features=128, out_features=10, bias=True))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_layers(fact_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283581"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(fact_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_count = count_parameters(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5085359424146675"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(fact_model)/count_parameters(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4375, 0.5625, 0.75, 0.65625, 0.09375]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48aac2a9a49c39ebb4503a423316524ff978d67c54926a1d8595b999b29100c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
